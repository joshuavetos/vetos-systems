#!/usr/bin/env python3
# ==========================================
# PURE SEMANTIC AUDITOR v3.3 (ROBUST)
# Production-grade | Dead URL handling | Epistemic boundary
# ==========================================

"""
Tests semantic cluster stability in support tickets. 
Refuses to analyze unstable data or missing inputs.
"""

import os
import sys
import warnings
import pandas as pd
import numpy as np
from collections import Counter
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# ENVIRONMENT VALIDATION (no mutation)
print("üîç SEMANTIC AUDITOR v3.3 | Production diagnostic")

try:
    assert pd.__version__ >= '2.2', "pandas>=2.2 required"
    import umap; import hdbscan
    from sentence_transformers import SentenceTransformer
    print("‚úÖ Environment validated")
except (AssertionError, ImportError) as e:
    print(f"‚ùå ENVIRONMENT: {e}")
    print("Required: pip install numpy>=2.0 pandas>=2.2 sentence-transformers umap-learn hdbscan")
    sys.exit(1)

warnings.filterwarnings('ignore')

# ==========================================
# 1. DATA VALIDATION w/ FALLBACK (ROBUST)
# ==========================================
print("\n1. DATA VALIDATION")
data_sources = [
    "https://raw.githubusercontent.com/Lawrence-Krukrubo/Customer_Support_Ticket_Analysis/master/customer_support_tickets.csv",
    # Dead URL handled gracefully
]

df = None
for i, url in enumerate(data_sources, 1):
    try:
        print(f"   Trying source {i}...")
        temp_df = pd.read_csv(url).head(150)
        text_col = 'Ticket Description' if 'Ticket Description' in temp_df.columns else temp_df.columns[0]
        df = temp_df.rename(columns={text_col: 'text'}).dropna(subset=['text'])
        if len(df) >= 20:
            print(f"‚úÖ Source {i}: {len(df)} valid tickets")
            break
    except:
        continue

# CRITICAL FALLBACK: Synthetic minimum viable dataset
if df is None or len(df) < 20:
    print("‚ö†Ô∏è  All sources failed ‚Üí Using minimum viable synthetic data")
    synthetic_tickets = [
        "can't login to portal", "portal access denied", "unable to access dashboard",
        "reset my password", "forgot password link broken", "password reset failed",
        "billing error on invoice", "charge appeared twice", "unexpected invoice",
        "payment failed", "card declined", "payment not processing",
        "api returning 500 error", "api endpoint down", "500 internal server error"
    ] * 10  # Exactly 120 tickets
    df = pd.DataFrame({"text": synthetic_tickets})
    print(f"‚úÖ Synthetic: {len(df)} tickets (for demo/stability testing)")

if len(df) < 20:
    print("‚ùå INSUFFICIENT DATA (<20 tickets)")
    print("ABSTAIN - cannot proceed")
    sys.exit(1)

# ==========================================
# 2. EMBEDDING STABILITY (O(1))
# ==========================================
print("\n2. EMBEDDING QUALITY")
model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(df['text'].tolist(), show_progress_bar=False)

# Sampled self-similarity MAD
n_samples = min(1000, len(embeddings))
sample_idx = np.random.choice(len(embeddings), n_samples, replace=False)
cosine_sims = np.diag(np.dot(embeddings[sample_idx], embeddings[sample_idx].T))
mad = np.median(np.abs(cosine_sims - np.median(cosine_sims)))
print(f"   Self-similarity MAD: {mad:.3f} {'‚úÖ' if mad < 0.15 else '‚ö†Ô∏è'}")

# ==========================================
# 3. BASELINE CLUSTERING
# ==========================================
print("\n3. BASELINE CLUSTERING")
reducer = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine', random_state=42)
embed_2d = reducer.fit_transform(embeddings)

clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3)
baseline_labels = clusterer.fit_predict(embed_2d)

n_clusters = len(np.unique(baseline_labels[baseline_labels > -1]))
n_noise = np.sum(baseline_labels == -1)
noise_ratio = n_noise / len(baseline_labels)

print(f"   Clusters: {n_clusters} | Noise: {noise_ratio*100:.1f}%")

# ==========================================
# 4. PRODUCTION STABILITY TESTS (NON-NOISE ONLY)
# ==========================================
print("\n4. STABILITY TESTS")
stability_passed = True
stability_results = []

baseline_non_noise = baseline_labels != -1

# TEST 1: Noise tolerance
print("   4.1 Noise tolerance...")
noisy_emb = embeddings + np.random.normal(0, 0.05, embeddings.shape)
noisy_2d = umap.UMAP(n_neighbors=15, random_state=43).fit_transform(noisy_emb)
noisy_labels = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3).fit_predict(noisy_2d)
if np.sum(baseline_non_noise) > 0:
    drift1 = np.mean(baseline_labels[baseline_non_noise] != noisy_labels[baseline_non_noise])
    print(f"      Non-noise drift: {drift1:.1%} {'‚úÖ' if drift1 < 0.15 else '‚ùå'}")
    stability_results.append(f"Noise:{drift1:.1%}")
    if drift1 >= 0.15: stability_passed = False
else:
    stability_results.append("Noise:N/A")

# TEST 2: Parameter sensitivity
print("   4.2 Parameter sensitivity...")
robust_2d = umap.UMAP(n_neighbors=10, n_components=2, random_state=42).fit_transform(embeddings)
robust_labels = hdbscan.HDBSCAN(min_cluster_size=7, min_samples=3).fit_predict(robust_2d)
if np.sum(baseline_non_noise) > 0:
    drift2 = np.mean(baseline_labels[baseline_non_noise] != robust_labels[baseline_non_noise])
    print(f"      Parameter drift: {drift2:.1%} {'‚úÖ' if drift2 < 0.15 else '‚ùå'}")
    stability_results.append(f"Param:{drift2:.1%}")
    if drift2 >= 0.15: stability_passed = False
else:
    stability_results.append("Param:N/A")

# TEST 3: Order invariance
print("   4.3 Order invariance...")
shuffle_idx = np.random.permutation(len(embeddings))
shuffle_emb = embeddings[shuffle_idx]
shuffle_2d = umap.UMAP(n_neighbors=15, random_state=42).fit_transform(shuffle_emb)
shuffle_labels = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3).fit_predict(shuffle_2d)
if np.sum(baseline_non_noise) > 0:
    assignment_drift = np.mean(baseline_labels[baseline_non_noise] != shuffle_labels[shuffle_idx][baseline_non_noise])
    print(f"      Order drift: {assignment_drift:.1%} {'‚úÖ' if assignment_drift < 0.20 else '‚ùå'}")
    stability_results.append(f"Order:{assignment_drift:.1%}")
    if assignment_drift >= 0.20: stability_passed = False
else:
    stability_results.append("Order:N/A")

# ==========================================
# 5. STATISTICAL SUMMARY
# ==========================================
df['cluster'] = baseline_labels

if stability_passed and n_clusters > 0 and noise_ratio < 0.3:
    clusters = df[df['cluster'] != -1].groupby('cluster').agg({
        'text': ['count', lambda x: ' | '.join(x.str.lower().str[:50].tolist()[:3])],
    }).round(0)
    clusters.columns = ['TicketShare', 'SampleText']
    clusters['TicketShare'] = clusters['TicketShare'] / clusters['TicketShare'].sum() * 100
    clusters['MedianLen'] = df[df['cluster'] != -1].groupby('cluster')['text'].str.len().median()
    
    print("\n5. PRODUCTION SUMMARY")
    print("="*80)
    print(clusters[['TicketShare', 'MedianLen', 'SampleText']].round(1).to_markdown())
    reliable_clusters = sum(clusters['TicketShare'] > 2)
else:
    print("\n5. NO RELIABLE CLUSTERS")
    clusters = pd.DataFrame()
    reliable_clusters = 0

# ==========================================
# 6. DECISION GATE
# ==========================================
print("\n6. PRODUCTION DECISION")
print(f"   Noise: {noise_ratio*100:.1f}%")
print(f"   Stability: {'‚úÖ PASSED' if stability_passed else '‚ùå FAILED'}")
print(f"   Reliable clusters: {reliable_clusters}/{n_clusters}")
print(f"   Tests: {', '.join(stability_results)}")

if stability_passed and noise_ratio < 0.3 and reliable_clusters >= 1:
    DECISION = "‚úÖ PRODUCTION READY"
    NEXT_STEPS = "Manual review of SampleText"
elif noise_ratio >= 0.3:
    DECISION = "‚ö†Ô∏è HIGH NOISE"
    NEXT_STEPS = "Cannot segment reliably"
else:
    DECISION = "‚ùå UNSTABLE"
    NEXT_STEPS = "Clusters too fragile"

print(f"\nüéØ DECISION: {DECISION}")
print(f"üìã NEXT: {NEXT_STEPS}")

# REFERENCE PLOT (OPT-IN)
if input("\nReference plot? (y/N): ").lower() == 'y':
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=embed_2d[:, 0], y=embed_2d[:, 1], hue=baseline_labels, 
                    palette='viridis', s=50, alpha=0.7)
    plt.title("REFERENCE ONLY - Statistical table is source of truth")
    plt.tight_layout()
    plt.savefig('clusters_reference.png', dpi=150, bbox_inches='tight')
    plt.show()

# AUDIT TRAIL
if DECISION == "‚úÖ PRODUCTION READY":
    df.to_csv('semantic_audit_v3.3.csv', index=False)
    if len(clusters) > 0:
        clusters.to_csv('cluster_summary_v3.3.csv', index=True)
    print("\nüíæ AUDIT TRAIL:")
    print("   ‚Ä¢ semantic_audit_v3.3.csv (all tickets + labels)")
    print("   ‚Ä¢ cluster_summary_v3.3.csv (decision record)")

print(f"\nüéØ v3.3 COMPLETE | {DECISION}")
